{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ef09d5-9174-4400-a47b-4f9d848740c0",
   "metadata": {},
   "source": [
    "# Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "862b2eff-0469-4a3b-ab34-3e6066e55b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "import unittest\n",
    "\n",
    "class ExampleTest(unittest.TestCase):\n",
    "    def test_something(self):\n",
    "        val = 1\n",
    "        self.assertEqual(val, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91c68ada-b68c-4ae1-befd-9c3d015633fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_something (test_example.ExampleTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Source\\itermediate-python\\content\\test_example.py\", line 6, in test_something\n",
      "    self.assertEqual(val, 5)\n",
      "AssertionError: 1 != 5\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251d9fe-4bc8-4a7f-a52f-31248a9e5be9",
   "metadata": {},
   "source": [
    "Similar thing in pytest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38fd0a98-c5d9-448e-bd00-e7fded8d9aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "def test_something():\n",
    "    val = 1\n",
    "    assert x == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3aea6723-37e4-4967-9399-ce25c5ac532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 1 item\n",
      "\n",
      "test_example.py \u001b[31mF\u001b[0m\u001b[31m                                                        [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_something ________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_something\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        val = \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m x == \u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NameError: name 'x' is not defined\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_example.py\u001b[0m:3: NameError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_example.py::\u001b[1mtest_something\u001b[0m - NameError: name 'x' is not defined\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.14s\u001b[0m\u001b[31m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca98315-5b08-4a93-95e5-9e621946f4be",
   "metadata": {},
   "source": [
    "1. Pytest provides clear breakdowns of test results, including the value of `val`, even though it uses the Python assert statement. This allows for easy identification of test failures and their specific details.\n",
    "\n",
    "2. Pytest does not require setting up a class for test cases, although you have the option to use classes if needed. This makes writing tests more straightforward and less verbose compared to some other testing frameworks.\n",
    "\n",
    "3. Unlike other testing frameworks that have numerous `self.assert*` functions for different assertions, pytest simplifies the process by using the familiar Python assert statement for making assertions in tests.\n",
    "\n",
    "4. Pytest is versatile and can run not only pytest-specific tests but also unittest tests and tests written for the old nose package. This makes it compatible with various testing frameworks, providing flexibility to developers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa32472-ae68-469f-bc60-23f1a5a38204",
   "metadata": {},
   "source": [
    "## Practice TDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1132e4b0-ddd6-486a-bbf2-eb3315046d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "def add(num1, num2):\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_add():\n",
    "    assert add(1, 2) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60b589e2-2ee4-4377-82e5-ad26e0a4f727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 1 item\n",
      "\n",
      "test_example.py \u001b[31mF\u001b[0m\u001b[31m                                                        [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________________ test_add ___________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_add\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m add(\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m) == \u001b[94m3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert None == 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where None = add(1, 2)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_example.py\u001b[0m:6: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_example.py::\u001b[1mtest_add\u001b[0m - assert None == 3\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.16s\u001b[0m\u001b[31m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6965ab16-7184-4672-bbdc-b15ee11be477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "def add(num1, num2):\n",
    "    return num1 + num2\n",
    "\n",
    "\n",
    "def test_add():\n",
    "    assert add(1, 2) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9030ed5f-9e66-42cb-b0c4-6e2f94098a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 1 item\n",
      "\n",
      "test_example.py \u001b[32m.\u001b[0m\u001b[32m                                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc24de2-1c49-428e-a097-932782732474",
   "metadata": {},
   "source": [
    "## Use `pytest.approx` for float operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "498cb615-4747-415a-b2e2-9ba20e2d6151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "def add(num1, num2):\n",
    "    return num1 + num2\n",
    "\n",
    "\n",
    "def test_add():\n",
    "    assert add(1, 2) == 3\n",
    "\n",
    "def test_add_float():\n",
    "    assert add(0.1, 0.2) == 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9e4f237-09ed-4170-8113-1f7fea35bfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 2 items\n",
      "\n",
      "test_example.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                       [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_add_float ________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_add_float\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m add(\u001b[94m0.1\u001b[39;49;00m, \u001b[94m0.2\u001b[39;49;00m) == \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 0.30000000000000004 == 0.3\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 0.30000000000000004 = add(0.1, 0.2)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_example.py\u001b[0m:9: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_example.py::\u001b[1mtest_add_float\u001b[0m - assert 0.30000000000000004 == 0.3\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.17s\u001b[0m\u001b[31m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8957e45-7538-42d7-b1d9-30c2e1d08368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "def add(num1, num2):\n",
    "    return num1 + num2\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_add():\n",
    "    assert add(1, 2) == 3\n",
    "\n",
    "def test_add_float():\n",
    "    assert add(0.1, 0.2) == pytest.approx(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19574185-8d1c-4a95-a729-4e2b51b08502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 2 items\n",
      "\n",
      "test_example.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be61f3-47d3-47ff-bc98-6006aa56f2d3",
   "metadata": {},
   "source": [
    "## Use `pytest.mark.parametrize` for handling unit test which have similar parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e3735bb-d685-40ab-b1a6-179097ac44e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "def add(num1, num2):\n",
    "    return num1 + num2\n",
    "\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    ('input_arg', 'expected'),\n",
    "    (\n",
    "        ((1, 2), 3),\n",
    "        ((0.1, 0.2), 0.3),\n",
    "    ),\n",
    ")\n",
    "def test_add(input_arg, expected):\n",
    "    assert add(*input_arg) == pytest.approx(expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b60721e-f8c6-4a1b-82a6-e575e444a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 2 items\n",
      "\n",
      "test_example.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0d62060-a4ab-4d26-bb73-7a4ed238a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0 -- C:\\Python38\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_example.py::test_add[input_arg0-3] \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 50%]\u001b[0m\n",
      "test_example.py::test_add[input_arg1-0.3] \u001b[32mPASSED\u001b[0m\u001b[32m                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# pytest by default is very silent, you can make it verbose b y passing -v command.\n",
    "!python -m pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e94fb-ba01-4557-b3af-e2720a98dc90",
   "metadata": {},
   "source": [
    "By default any print statement inside test function will not show up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "252781d2-a153-42da-9dc7-9345aa9f3b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "def add(num1, num2):\n",
    "    return num1 + num2\n",
    "\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    ('input_arg', 'expected'),\n",
    "    (\n",
    "        ((1, 2), 3),\n",
    "        ((0.1, 0.2), 0.3),\n",
    "    ),\n",
    ")\n",
    "def test_add(input_arg, expected):\n",
    "    print(f\"\\n{input_arg = }, {expected = }\")\n",
    "    assert add(*input_arg) == pytest.approx(expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "306c39f3-da4a-4847-8fbc-813ce6165d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0 -- C:\\Python38\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_example.py::test_add[input_arg0-3] \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 50%]\u001b[0m\n",
      "test_example.py::test_add[input_arg1-0.3] \u001b[32mPASSED\u001b[0m\u001b[32m                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb1adf22-20ff-496d-b3bd-3ec1ef96cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0 -- C:\\Python38\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_example.py::test_add[input_arg0-3] \n",
      "input_arg = (1, 2), expected = 3\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "test_example.py::test_add[input_arg1-0.3] \n",
      "input_arg = (0.1, 0.2), expected = 0.3\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -vs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd106f0-7ed9-472d-8e74-8936a30a461a",
   "metadata": {},
   "source": [
    "Use `pytest.raises` to check if an expected exception is raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0f0f4466-24a3-4e88-8961-0977722efd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "import pytest\n",
    "\n",
    "def test_raises():\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52403a02-1958-4e02-93b9-c2579df1cb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 1 item\n",
      "\n",
      "test_example.py \u001b[32m.\u001b[0m\u001b[32m                                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7094e-7541-4e5a-97ed-ad1e5ee4d819",
   "metadata": {},
   "source": [
    "## pytest fixtures\n",
    "\n",
    "- Pytest fixtures are functions used in tests for setup, teardown, and handling resources.\n",
    "- They are recognized by names in test functions and automatically invoked by pytest.\n",
    "- Fixtures centralize setup logic, making tests modular, reusable, and maintainable.\n",
    "- Pytest provides built-in fixtures and allows custom fixtures to enhance testing capabilities.\n",
    "- Fixtures support parameterization and different scopes for precise control over their usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c222a5-f1b6-46c5-9c30-c0dc447472af",
   "metadata": {},
   "source": [
    "### fixture as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de096fb9-fba3-4a2d-ad88-3db4605212fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "class Calculator:\n",
    "    def add(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "    def subtract(self, a, b):\n",
    "        return a - b\n",
    "\n",
    "def test_addition():\n",
    "    calc = Calculator()\n",
    "    result = calc.add(5, 10)\n",
    "    assert result == 15\n",
    "\n",
    "def test_subtraction():\n",
    "    calc = Calculator()\n",
    "    result = calc.subtract(20, 8)\n",
    "    assert result == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de85eb75-40e2-405e-b345-34a03a05e5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 2 items\n",
      "\n",
      "test_example.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "041a810b-0398-48de-8cac-0efc1e1ba7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "class Calculator:\n",
    "    def add(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "    def subtract(self, a, b):\n",
    "        return a - b\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def calc():\n",
    "    return Calculator()\n",
    "\n",
    "def test_addition(calc):\n",
    "    result = calc.add(5, 10)\n",
    "    assert result == 15\n",
    "\n",
    "def test_subtraction(calc):\n",
    "    result = calc.subtract(20, 8)\n",
    "    assert result == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e423b73e-3181-4590-a00d-9b2c699c7204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 2 items\n",
      "\n",
      "test_example.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279afd23-f274-4820-b59f-7f1ac5117273",
   "metadata": {},
   "source": [
    "### fixture as test state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e0fd8fa-1653-4d62-a319-87307db52a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def setup_and_teardown_example():\n",
    "    print(\"Setup - Before the test\")\n",
    "    yield\n",
    "    print(\"Teardown - After the test\")\n",
    "\n",
    "def test_add(setup_and_teardown_example):\n",
    "    print(\"Running test_add\")\n",
    "    assert 1 + 1 == 2\n",
    "\n",
    "@pytest.mark.usefixtures('setup_and_teardown_example')\n",
    "def test_sub():\n",
    "    print(\"Running test_sub\")\n",
    "    assert 1 - 1 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c2035a65-2d63-468d-b518-4447f5b85f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 2 items\n",
      "\n",
      "test_example.py Setup - Before the test\n",
      "Running test_add\n",
      "\u001b[32m.\u001b[0mTeardown - After the test\n",
      "Setup - Before the test\n",
      "Running test_sub\n",
      "\u001b[32m.\u001b[0mTeardown - After the test\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda167fa-4b50-42e2-a87e-0b9390e568ba",
   "metadata": {},
   "source": [
    "### run fixture for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f41c08b3-83c6-4129-9195-d75cc237ac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def setup_and_teardown_example():\n",
    "    print(\"Setup - Before the test\")\n",
    "    yield\n",
    "    print(\"Teardown - After the test\")\n",
    "\n",
    "def test_add():\n",
    "    print(\"Running test_add\")\n",
    "    assert 1 + 1 == 2\n",
    "\n",
    "def test_sub():\n",
    "    print(\"Running test_sub\")\n",
    "    assert 1 - 1 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1487a97f-8a4f-45b3-bed9-d2ac476f0350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 2 items\n",
      "\n",
      "test_example.py Setup - Before the test\n",
      "Running test_add\n",
      "\u001b[32m.\u001b[0mRunning test_sub\n",
      "\u001b[32m.\u001b[0mTeardown - After the test\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc224ec-c8e3-4439-aa31-28d76100433a",
   "metadata": {},
   "source": [
    "By default the fixture is scope in function. So for each function the fixture is invoked. But we can set it to some specific scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f7d77a4e-1866-4c4a-8296-ce14e3a00950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_example.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(autouse=True, scope='session')\n",
    "def setup_and_teardown_example():\n",
    "    print(\"Setup - Before the test\")\n",
    "    yield\n",
    "    print(\"Teardown - After the test\")\n",
    "\n",
    "def test_add():\n",
    "    print(\"Running test_add\")\n",
    "    assert 1 + 1 == 2\n",
    "\n",
    "def test_sub():\n",
    "    print(\"Running test_sub\")\n",
    "    assert 1 - 1 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a4754186-7257-493a-b42f-55b6e12f8054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.8.8, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: C:\\Source\\itermediate-python\\content\n",
      "plugins: anyio-3.6.2, cov-4.0.0, returns-0.19.0\n",
      "collected 2 items\n",
      "\n",
      "test_example.py Setup - Before the test\n",
      "Running test_add\n",
      "\u001b[32m.\u001b[0mRunning test_sub\n",
      "\u001b[32m.\u001b[0mTeardown - After the test\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest -s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
